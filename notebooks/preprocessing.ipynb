{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "_ = pl.Config.set_tbl_rows(10)\n",
    "_ = pl.Config.set_tbl_cols(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdpc.data import (\n",
    "    joined_dex_swaps_df,\n",
    "    joined_token_transfers_df,\n",
    "    joined_train_df,\n",
    "    joined_transactions_df,\n",
    "    test_data_df,\n",
    "    wallet_addresses_df,\n",
    ")\n",
    "\n",
    "addresses: pl.DataFrame = wallet_addresses_df()\n",
    "train_df: pl.DataFrame = joined_train_df()\n",
    "transactions_df: pl.DataFrame = joined_transactions_df()\n",
    "dex_swaps_df: pl.DataFrame = joined_dex_swaps_df()\n",
    "token_transfers_df: pl.DataFrame = joined_token_transfers_df()\n",
    "test_df: pl.DataFrame = test_data_df()\n",
    "\n",
    "# Features start with addresses\n",
    "features_df = addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats of unique addresses in train and test datasets\n",
    "train_addresses = train_df.select(\"address\").unique()\n",
    "test_addresses = test_df.select(\"address\").unique()\n",
    "\n",
    "print(f\"Number of unique addresses in train dataset: {train_addresses.height}\")\n",
    "print(f\"Number of unique addresses in test dataset: {test_addresses.height}\")\n",
    "\n",
    "# Check for any overlap between train and test addresses\n",
    "overlap = train_addresses.join(test_addresses, on=\"address\", how=\"inner\")\n",
    "print(f\"Number of addresses that appear in both train and test: {overlap.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_data = pl.read_csv(\n",
    "    \"../data/processed/addresses_community_simple.csv\"\n",
    ").with_columns(\n",
    "    pl.col(\"address\").count().over(\"community\").alias(\"community_size\"),\n",
    ")\n",
    "\n",
    "features_df = features_df.join(community_data, on=\"address\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network metrics data\n",
    "network_metrics = pl.read_csv(\"../data/processed/network_metrics.csv\").drop(\n",
    "    \"label\", \"split\"\n",
    ")\n",
    "\n",
    "# Join network metrics with features dataframe\n",
    "features_df = features_df.join(network_metrics, on=\"address\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node2vec embeddings\n",
    "node2vec_embeddings = pl.read_parquet(\n",
    "    \"../data/processed/node2vec_embeddings.parquet\"\n",
    ").drop(\"label\", \"split\")\n",
    "\n",
    "# Join node2vec embeddings with features dataframe\n",
    "features_df = features_df.join(node2vec_embeddings, on=\"address\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flipside Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipside_addresses_labels: pl.DataFrame = pl.read_parquet(\n",
    "    \"../data/external/flipside_address_labels.parquet\"\n",
    ")\n",
    "\n",
    "features_df = features_df.join(\n",
    "    flipside_addresses_labels,\n",
    "    on=\"address\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipside_contracts_labels: pl.DataFrame = pl.read_parquet(\n",
    "    \"../data/external/flipside_contracts_data.parquet\"\n",
    ")\n",
    "\n",
    "flipside_contracts = flipside_contracts_labels.get_column(\"address\")\n",
    "flipside_contracts_creators = flipside_contracts_labels.get_column(\"creator_address\")\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\").is_in(flipside_contracts).alias(\"flipside_is_contract\"),\n",
    "    pl.col(\"address\")\n",
    "    .is_in(flipside_contracts_creators)\n",
    "    .alias(\"flipside_is_contract_creator\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Known Sybil Lists Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zk_cluster_list = (\n",
    "    pl.read_csv(\"../data/external/zk_cluster_list.csv\", ignore_errors=True)\n",
    "    .select(pl.col(\"Wallet Address\").alias(\"address\"))\n",
    "    .filter(pl.col(\"address\").str.starts_with(\"0x\"))\n",
    ")\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(zk_cluster_list.get_column(\"address\"))\n",
    "    .alias(\"zk_cluster_list_hit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the ZKSync sybil list\n",
    "all_zksync_sybil_list = pl.DataFrame()\n",
    "for file in [\n",
    "    \"../data/external/zksync_sybil_list_0.csv\",\n",
    "    \"../data/external/zksync_sybil_list_1.csv\",\n",
    "    \"../data/external/zksync_sybil_list_2.csv\",\n",
    "]:\n",
    "    zksync_sybil_list = pl.read_csv(file)\n",
    "    all_zksync_sybil_list = pl.concat([all_zksync_sybil_list, zksync_sybil_list])\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(all_zksync_sybil_list.get_column(\"userId\"))\n",
    "    .alias(\"zksync_sybil_list_hit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Layer Zero wallet list\n",
    "layer_zero_wallet_list = pl.read_csv(\"../data/external/layer_zero_wallet_list.csv\")\n",
    "layer_zero_wallet_addresses = layer_zero_wallet_list.get_column(\"ADDRESS\")\n",
    "\n",
    "# Add a column indicating if the address is in the Layer Zero wallet list\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(layer_zero_wallet_addresses)\n",
    "    .alias(\"layer_zero_wallet_list_hit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CT App LZ list\n",
    "ct_app_lz_list = pl.read_parquet(\"../data/external/ct_app_lz_list.parquet\")\n",
    "\n",
    "# Add a column indicating if the address is in the CT App LZ list\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(ct_app_lz_list.get_column(\"Line\"))\n",
    "    .alias(\"ct_app_lz_list_hit\")\n",
    ")\n",
    "\n",
    "# Read the CT App LZ list\n",
    "ct_app_bn_wl = pl.read_parquet(\"../data/external/ct_app_bn_wl.parquet\")\n",
    "\n",
    "# Add a column indicating if the address is in the CT App LZ list\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(ct_app_bn_wl.get_column(\"Address\"))\n",
    "    .alias(\"ct_app_bn_wl_hit\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lz_initial_list = pl.read_parquet(\"../data/external/lz_initial_list.parquet\")\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(lz_initial_list.get_column(\"ADDRESS\"))\n",
    "    .alias(\"lz_initial_list_hit\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the LZ provisional sybil list\n",
    "lz_provisional_sybil_list = pl.read_parquet(\n",
    "    \"../data/external/lz_provisional_sybil_list.parquet\"\n",
    ")\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(lz_provisional_sybil_list.get_column(\"address\"))\n",
    "    .alias(\"lz_provisional_sybil_list_hit\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Hop sybil list\n",
    "hop_sybil_list = pl.read_csv(\"../data/external/hop_sybils.csv\")\n",
    "\n",
    "# Add a column indicating if the address is in the Hop sybil list\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(hop_sybil_list.get_column(\"address\"))\n",
    "    .alias(\"hop_sybil_list_hit\")\n",
    ")\n",
    "\n",
    "# Read the Hop sybil list\n",
    "hop_all_data_filtered = pl.read_csv(\"../data/external/hop_all_data_filtered.csv\")\n",
    "\n",
    "# Add a column indicating if the address is in the Hop sybil list\n",
    "features_df = features_df.with_columns(\n",
    "    pl.col(\"address\")\n",
    "    .is_in(hop_all_data_filtered.get_column(\"Wallet\"))\n",
    "    .alias(\"hop_all_data_filtered_hit\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sybil Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.join(\n",
    "    train_df.select([\"address\", \"label\"]),\n",
    "    on=\"address\",\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = (\n",
    "    transactions_df.select(\n",
    "        pl.col(\"BLOCK_NUMBER\").alias(\"block_number\"),\n",
    "        pl.col(\"BLOCK_TIMESTAMP\").alias(\"block_timestamp\"),\n",
    "        pl.col(\"TX_HASH\").alias(\"tx_hash\"),\n",
    "        pl.col(\"FROM_ADDRESS\").alias(\"from_address\"),\n",
    "        pl.col(\"TO_ADDRESS\").alias(\"to_address\"),\n",
    "        pl.col(\"VALUE\").alias(\"value\"),\n",
    "        pl.col(\"TX_FEE\").alias(\"tx_fee\"),\n",
    "        pl.col(\"GAS_PRICE\").alias(\"gas_price\"),\n",
    "        pl.col(\"GAS_LIMIT\").alias(\"gas_limit\"),\n",
    "        pl.col(\"GAS_USED\").alias(\"gas_used\"),\n",
    "        pl.col(\"INPUT_DATA\").alias(\"input_data\"),\n",
    "        pl.col(\"NETWORK\").alias(\"network\"),\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"from_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"to_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_to\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_aggregations = [\n",
    "    pl.col(\"block_number\").n_unique().alias(\"unique_block_numbers\"),\n",
    "    pl.col(\"block_timestamp\").min().alias(\"min_block_timestamp\"),\n",
    "    pl.col(\"block_timestamp\").max().alias(\"max_block_timestamp\"),\n",
    "    pl.col(\"tx_hash\").n_unique().alias(\"unique_tx_hashes\"),\n",
    "    pl.col(\"value\").sum().alias(\"total_value\"),\n",
    "    pl.col(\"value\").mean().alias(\"avg_value\"),\n",
    "    pl.col(\"value\").max().alias(\"max_value\"),\n",
    "    pl.col(\"value\").min().alias(\"min_value\"),\n",
    "    pl.col(\"tx_fee\").sum().alias(\"total_tx_fee\"),\n",
    "    pl.col(\"tx_fee\").mean().alias(\"avg_tx_fee\"),\n",
    "    pl.col(\"gas_price\").sum().cast(pl.Int64).alias(\"total_gas_price\"),\n",
    "    pl.col(\"gas_price\").mean().cast(pl.Int64).alias(\"avg_gas_price\"),\n",
    "    pl.col(\"gas_limit\").sum().cast(pl.Int64).alias(\"total_gas_limit\"),\n",
    "    pl.col(\"gas_limit\").mean().cast(pl.Int64).alias(\"avg_gas_limit\"),\n",
    "    pl.col(\"gas_used\").sum().cast(pl.Int64).alias(\"total_gas_used\"),\n",
    "    pl.col(\"gas_used\").mean().cast(pl.Int64).alias(\"avg_gas_used\"),\n",
    "    pl.col(\"network\").n_unique().alias(\"unique_networks\"),\n",
    "]\n",
    "\n",
    "from_aggregations = [\n",
    "    pl.col(\"from_address\").n_unique().alias(\"unique_from_addresses\"),\n",
    "    pl.col(\"value\").sort_by(\"block_timestamp\").first().alias(\"first_tx_from_value\"),\n",
    "    # pl.col(\"passport_stamps_score\").mean().alias(\"avg_passport_stamps_score\"),\n",
    "    pl.col(\"flipside_address_name\").n_unique().alias(\"address_name_count\"),\n",
    "    pl.col(\"flipside_is_contract\").mean().alias(\"avg_flipside_is_contract\"),\n",
    "    pl.col(\"flipside_is_contract\").sum().alias(\"flipside_is_contract_count\"),\n",
    "    pl.col(\"community\").n_unique().alias(\"unique_communities\"),\n",
    "    pl.col(\"community_size\").mean().alias(\"avg_community_size\"),\n",
    "]\n",
    "\n",
    "to_aggregations = [\n",
    "    pl.col(\"to_address\").n_unique().alias(\"unique_to_addresses\"),\n",
    "    pl.col(\"value\").sort_by(\"block_timestamp\").first().alias(\"first_tx_to_value\"),\n",
    "    # pl.col(\"passport_stamps_score_to\").mean().alias(\"avg_passport_stamps_score_to\"),\n",
    "    pl.col(\"flipside_address_name_to\").n_unique().alias(\"address_name_count_to\"),\n",
    "    pl.col(\"flipside_is_contract_to\").mean().alias(\"avg_flipside_is_contract_to\"),\n",
    "    pl.col(\"flipside_is_contract_to\").sum().alias(\"flipside_is_contract_count_to\"),\n",
    "    pl.col(\"community\").n_unique().alias(\"unique_communities_to\"),\n",
    "    pl.col(\"community_size\").mean().alias(\"avg_community_size_to\"),\n",
    "]\n",
    "\n",
    "from_all_metrics_df = (\n",
    "    transactions_df.group_by([\"from_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *to_aggregations,\n",
    "        pl.col(\"tx_hash\")\n",
    "        .filter(pl.col(\"label_to\") == 1)\n",
    "        .n_unique()\n",
    "        .alias(\"num_transactions_to_sybil\"),\n",
    "        pl.col(\"to_address\")\n",
    "        .filter(pl.col(\"label_to\") == 1)\n",
    "        .n_unique()\n",
    "        .alias(\"num_unique_to_sybil_addresses\"),\n",
    "    )\n",
    "    .rename({\"from_address\": \"address\"})\n",
    ")\n",
    "\n",
    "to_all_metrics_df = (\n",
    "    transactions_df.group_by([\"to_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *from_aggregations,\n",
    "        pl.col(\"tx_hash\")\n",
    "        .filter(pl.col(\"label\") == 1)\n",
    "        .n_unique()\n",
    "        .alias(\"num_transactions_from_sybil\"),\n",
    "        pl.col(\"from_address\")\n",
    "        .filter(pl.col(\"label\") == 1)\n",
    "        .n_unique()\n",
    "        .alias(\"num_unique_from_sybil_addresses\"),\n",
    "    )\n",
    "    .rename({\"to_address\": \"address\"})\n",
    ")\n",
    "\n",
    "from_network_transactions_metrics_df = (\n",
    "    transactions_df.group_by([\"from_address\", \"network\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        pl.col(\"to_address\").n_unique().alias(\"unique_to_addresses\"),\n",
    "    )\n",
    "    .pivot(on=\"network\", index=\"from_address\")\n",
    ").rename({\"from_address\": \"address\"})\n",
    "\n",
    "to_network_transactions_metrics_df = (\n",
    "    transactions_df.group_by([\"to_address\", \"network\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        pl.col(\"from_address\").n_unique().alias(\"unique_from_addresses\"),\n",
    "    )\n",
    "    .pivot(on=\"network\", index=\"to_address\")\n",
    ").rename({\"to_address\": \"address\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEX Swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dex_swaps_df = (\n",
    "    dex_swaps_df.select(\n",
    "        pl.col(\"BLOCK_NUMBER\").alias(\"block_number\"),\n",
    "        pl.col(\"BLOCK_TIMESTAMP\").alias(\"block_timestamp\"),\n",
    "        pl.col(\"TX_HASH\").alias(\"tx_hash\"),\n",
    "        pl.col(\"ORIGIN_FROM_ADDRESS\").alias(\"origin_from_address\"),\n",
    "        pl.col(\"ORIGIN_TO_ADDRESS\").alias(\"origin_to_address\"),\n",
    "        pl.col(\"CONTRACT_ADDRESS\").alias(\"contract_address\"),\n",
    "        pl.col(\"POOL_NAME\").alias(\"pool_name\"),\n",
    "        pl.col(\"AMOUNT_IN_USD\").cast(pl.Int64).alias(\"amount_in_usd\"),\n",
    "        pl.col(\"AMOUNT_OUT_USD\").cast(pl.Int64).alias(\"amount_out_usd\"),\n",
    "        pl.col(\"SENDER\").alias(\"sender\"),\n",
    "        pl.col(\"TX_TO\").alias(\"tx_to\"),\n",
    "        pl.col(\"PLATFORM\").alias(\"platform\"),\n",
    "        pl.col(\"TOKEN_IN\").alias(\"token_in\"),\n",
    "        pl.col(\"TOKEN_OUT\").alias(\"token_out\"),\n",
    "        pl.col(\"SYMBOL_IN\").alias(\"symbol_in\"),\n",
    "        pl.col(\"SYMBOL_OUT\").alias(\"symbol_out\"),\n",
    "        pl.col(\"NETWORK\").alias(\"network\"),\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"origin_from_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"origin_to_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_to\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_aggregations = [\n",
    "    pl.col(\"tx_hash\").n_unique().alias(\"unique_tx_hashes\"),\n",
    "    pl.col(\"contract_address\").n_unique().alias(\"unique_contract_addresses\"),\n",
    "    pl.col(\"block_timestamp\").min().alias(\"min_block_timestamp\"),\n",
    "    pl.col(\"block_timestamp\").max().alias(\"max_block_timestamp\"),\n",
    "    pl.col(\"pool_name\").n_unique().alias(\"unique_pool_names\"),\n",
    "    pl.col(\"amount_in_usd\").sum().alias(\"total_amount_in_usd\"),\n",
    "    pl.col(\"amount_in_usd\").mean().alias(\"avg_amount_in_usd\"),\n",
    "    pl.col(\"amount_in_usd\").max().alias(\"max_amount_in_usd\"),\n",
    "    pl.col(\"amount_in_usd\").min().alias(\"min_amount_in_usd\"),\n",
    "    pl.col(\"amount_out_usd\").sum().alias(\"total_amount_out_usd\"),\n",
    "    pl.col(\"amount_out_usd\").mean().alias(\"avg_amount_out_usd\"),\n",
    "    pl.col(\"amount_out_usd\").max().alias(\"max_amount_out_usd\"),\n",
    "    pl.col(\"amount_out_usd\").min().alias(\"min_amount_out_usd\"),\n",
    "    pl.col(\"platform\").n_unique().alias(\"unique_platforms\"),\n",
    "    pl.col(\"platform\")\n",
    "    .value_counts()\n",
    "    .head(1)\n",
    "    .struct.field(\"platform\")\n",
    "    .first()\n",
    "    .alias(\"most_common_platform\"),\n",
    "    # pl.col(\"community\").n_unique().alias(\"unique_communities\"),\n",
    "    # pl.col(\"community_size\").mean().alias(\"avg_community_size\"),\n",
    "]\n",
    "\n",
    "from_aggregations = [\n",
    "    pl.col(\"origin_from_address\").n_unique().alias(\"unique_origin_from_addresses\")\n",
    "]\n",
    "to_aggregations = [\n",
    "    pl.col(\"origin_to_address\").n_unique().alias(\"unique_origin_to_addresses\")\n",
    "]\n",
    "\n",
    "dex_from_all_metrics_df = (\n",
    "    dex_swaps_df.group_by([\"origin_from_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *to_aggregations,\n",
    "    )\n",
    "    .rename({\"origin_from_address\": \"address\"})\n",
    ")\n",
    "\n",
    "dex_to_all_metrics_df = (\n",
    "    dex_swaps_df.group_by([\"origin_to_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *from_aggregations,\n",
    "    )\n",
    "    .rename({\"origin_to_address\": \"address\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transfers_df = (\n",
    "    token_transfers_df.select(\n",
    "        pl.col(\"BLOCK_NUMBER\").alias(\"block_number\"),\n",
    "        pl.col(\"BLOCK_TIMESTAMP\").alias(\"block_timestamp\"),\n",
    "        pl.col(\"TX_HASH\").alias(\"tx_hash\"),\n",
    "        pl.col(\"ORIGIN_FROM_ADDRESS\").alias(\"origin_from_address\"),\n",
    "        pl.col(\"ORIGIN_TO_ADDRESS\").alias(\"origin_to_address\"),\n",
    "        pl.col(\"CONTRACT_ADDRESS\").alias(\"contract_address\"),\n",
    "        pl.col(\"FROM_ADDRESS\").alias(\"from_address\"),\n",
    "        pl.col(\"TO_ADDRESS\").alias(\"to_address\"),\n",
    "        pl.col(\"AMOUNT_USD\").cast(pl.Int64, wrap_numerical=True).alias(\"amount_usd\"),\n",
    "        pl.col(\"SYMBOL\").alias(\"symbol\"),\n",
    "        pl.col(\"NETWORK\").alias(\"network\"),\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"from_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .join(\n",
    "        features_df,\n",
    "        left_on=\"to_address\",\n",
    "        right_on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_to\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_aggregations = [\n",
    "    pl.col(\"tx_hash\").n_unique().alias(\"unique_tx_hashes\"),\n",
    "    pl.col(\"contract_address\").n_unique().alias(\"unique_contract_addresses\"),\n",
    "    pl.col(\"block_timestamp\").min().alias(\"min_block_timestamp\"),\n",
    "    pl.col(\"block_timestamp\").max().alias(\"max_block_timestamp\"),\n",
    "    pl.col(\"symbol\").n_unique().alias(\"unique_symbols\"),\n",
    "    pl.col(\"amount_usd\").sum().alias(\"total_amount_usd\"),\n",
    "    pl.col(\"amount_usd\").mean().alias(\"avg_amount_usd\"),\n",
    "    pl.col(\"amount_usd\").max().alias(\"max_amount_usd\"),\n",
    "    pl.col(\"amount_usd\").min().alias(\"min_amount_usd\"),\n",
    "    pl.col(\"network\").n_unique().alias(\"unique_networks\"),\n",
    "    pl.col(\"symbol\")\n",
    "    .value_counts()\n",
    "    .head(1)\n",
    "    .struct.field(\"symbol\")\n",
    "    .first()\n",
    "    .alias(\"most_common_symbol\"),\n",
    "]\n",
    "\n",
    "to_aggregations = [\n",
    "    pl.col(\"to_address\").n_unique().alias(\"unique_to_addresses\"),\n",
    "]\n",
    "\n",
    "from_aggregations = [\n",
    "    pl.col(\"from_address\").n_unique().alias(\"unique_from_addresses\"),\n",
    "]\n",
    "\n",
    "token_transfers_from_all_metrics_df = (\n",
    "    token_transfers_df.group_by([\"from_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *to_aggregations,\n",
    "    )\n",
    "    .rename({\"from_address\": \"address\"})\n",
    ")\n",
    "\n",
    "token_transfers_to_all_metrics_df = (\n",
    "    token_transfers_df.group_by([\"to_address\"])\n",
    "    .agg(\n",
    "        *common_aggregations,\n",
    "        *from_aggregations,\n",
    "    )\n",
    "    .rename({\"to_address\": \"address\"})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (\n",
    "    features_df.join(\n",
    "        from_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_from_all\",\n",
    "    )\n",
    "    .join(\n",
    "        to_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_to_all\",\n",
    "    )\n",
    "    .join(\n",
    "        from_network_transactions_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_from_network\",\n",
    "    )\n",
    "    .join(\n",
    "        to_network_transactions_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_to_network\",\n",
    "    )\n",
    "    .join(\n",
    "        dex_from_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_dex_from_all\",\n",
    "    )\n",
    "    .join(\n",
    "        dex_to_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_dex_to_all\",\n",
    "    )\n",
    "    .join(\n",
    "        token_transfers_from_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_token_transfers_from_all\",\n",
    "    )\n",
    "    .join(\n",
    "        token_transfers_to_all_metrics_df,\n",
    "        on=\"address\",\n",
    "        how=\"left\",\n",
    "        suffix=\"_token_transfers_to_all\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs\n",
    "\n",
    "features_df = features_df.with_columns(\n",
    "    pl.min_horizontal(features_df.select(cs.datetime()).columns).alias(\n",
    "        \"first_interaction\"\n",
    "    ),\n",
    "    pl.max_horizontal(features_df.select(cs.datetime()).columns).alias(\n",
    "        \"last_interaction\"\n",
    "    ),\n",
    ").with_columns(\n",
    "    (pl.col(\"last_interaction\") - pl.col(\"first_interaction\"))\n",
    "    .dt.total_days()\n",
    "    .alias(\"interaction_duration\"),\n",
    "    (pl.datetime(2025, 4, 26) - pl.col(\"first_interaction\"))\n",
    "    .dt.total_days()\n",
    "    .alias(\"days_since_first_interaction\"),\n",
    "    (pl.datetime(2025, 4, 26) - pl.col(\"last_interaction\"))\n",
    "    .dt.total_days()\n",
    "    .alias(\"days_since_last_interaction\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.with_columns(\n",
    "    pl.sum_horizontal(pl.all().is_null()).alias(\"null_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.with_columns(\n",
    "    # 1. Total outgoing transactions (native, DEX, token) per day of interaction\n",
    "    (\n",
    "        (\n",
    "            pl.col(\"unique_tx_hashes\").fill_null(0)\n",
    "            + pl.col(\"unique_tx_hashes_dex_from_all\").fill_null(0)\n",
    "            + pl.col(\"unique_tx_hashes_token_transfers_from_all\").fill_null(0)\n",
    "        )\n",
    "        / (pl.col(\"interaction_duration\").fill_null(0) + 1.0)\n",
    "    ).alias(\"tx_per_day\"),\n",
    "    # 2. Average ETH value per outgoing native transaction\n",
    "    (\n",
    "        pl.col(\"total_value\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"avg_native_eth_out_value\"),\n",
    "    # 3. Average ETH value per incoming native transaction\n",
    "    (\n",
    "        pl.col(\"total_value_to_all\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes_to_all\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"avg_native_eth_in_value\"),\n",
    "    # 4. Ratio of count of outgoing native ETH transactions to incoming native ETH transactions\n",
    "    (\n",
    "        pl.col(\"unique_tx_hashes\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes_to_all\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"native_tx_flow_ratio\"),\n",
    "    # 5. Ratio of total native transaction fees to total native ETH value sent\n",
    "    (\n",
    "        pl.col(\"total_tx_fee\").fill_null(0)\n",
    "        / (pl.col(\"total_value\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"fee_to_value_ratio_native_out\"),\n",
    "    # 6. Proportion of outgoing native transactions sent to addresses labeled as Sybil\n",
    "    (\n",
    "        pl.col(\"num_transactions_to_sybil\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"outgoing_to_sybil_tx_ratio\"),\n",
    "    # 7. Average USD value of assets swapped in DEXs by the address\n",
    "    (\n",
    "        pl.col(\"total_amount_in_usd\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes_dex_from_all\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"avg_dex_swap_in_usd_value\"),\n",
    "    # 8. Average USD value of tokens transferred out by the address\n",
    "    (\n",
    "        pl.col(\"total_amount_usd\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes_token_transfers_from_all\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"avg_token_transfer_out_usd_value\"),\n",
    "    # 9. Proportion of outgoing native transactions on Base network\n",
    "    (\n",
    "        pl.col(\"unique_tx_hashes_base\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"proportion_base_native_tx_out\"),\n",
    "    # 10. Average number of unique recipients per outgoing native transaction\n",
    "    (\n",
    "        pl.col(\"unique_to_addresses\").fill_null(0)\n",
    "        / (pl.col(\"unique_tx_hashes\").fill_null(0) + 1e-9)\n",
    "    ).alias(\"recipient_diversity_native_out\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns that have only nulls in the train set\n",
    "train_df = features_df.filter(pl.col(\"split\") == \"train\")\n",
    "train_null_cols = [\n",
    "    col\n",
    "    for col in train_df.columns\n",
    "    if (train_df[col].is_null().sum() == train_df.height)\n",
    "]\n",
    "\n",
    "features_df = features_df.drop(train_null_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.shrink_to_fit().write_parquet(\n",
    "    \"../data/processed/features_df.parquet\", compression=\"zstd\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
